{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 11:24:31.261501: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 11:24:31.775265: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-29 11:24:31.778865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-29 11:24:33.861371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_PIPELINE_NAME = 'doc_to_sleep_schema'\n",
    "PIPELINE_NAME = 'doc_to_sleep'\n",
    "\n",
    "# Output directory to store artifacts generated from the pipelines.\n",
    "SCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME)\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "\n",
    "# Path to a SQLite database file to use as an MLMD storage.\n",
    "SCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME, 'metadata.db')\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO) # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = os.path.join('converted_data', 'poly3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a preliminary schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.metadata import Metadata, sqlite_metadata_connection_config\n",
    "\n",
    "def _create_schema_pipeline(pipeline_name: str,\n",
    "                            pipeline_root: str,\n",
    "                            data_root: str,\n",
    "                            metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "    '''Creates a pipeline for schema generation'''\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    # Computes statistics over data for visualization and schema generation.\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "\n",
    "    # Generates schema based on the generated statistics.\n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        infer_feature_shape=True\n",
    "    )\n",
    "\n",
    "    components = [example_gen,\n",
    "                  statistics_gen,\n",
    "                  schema_gen]\n",
    "    \n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        metadata_connection_config=sqlite_metadata_connection_config(metadata_path),\n",
    "        components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "    _create_schema_pipeline(\n",
    "        pipeline_name=SCHEMA_PIPELINE_NAME,\n",
    "        pipeline_root=SCHEMA_PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        metadata_path=SCHEMA_METADATA_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review outputs of the schema pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "    '''Output artifacts of the latest run of the component.'''\n",
    "    context = metadata.store.get_context_by_type_and_name(\n",
    "        'node', f'{pipeline_name}.{component_id}'\n",
    "    )\n",
    "    executions = metadata.store.get_executions_by_context(context.id)\n",
    "    latest_execution = max(executions,\n",
    "    key=lambda e:e.last_update_time_since_epoch)\n",
    "    return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import visualizations\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "    '''Visualizes artifacts using standard visualization modules.'''\n",
    "    for artifact in artifacts:\n",
    "        visualization = visualizations.get_registry().get_visualization(\n",
    "            artifact.type_name\n",
    "        )\n",
    "        if visualization:\n",
    "            visualization.display(artifact)\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
    "\n",
    "standard_visualizations.register_standard_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the outputs from the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.metadata import Metadata, sqlite_metadata_connection_config\n",
    "from tfx.types import standard_component_specs\n",
    "\n",
    "metadata_connection_config = sqlite_metadata_connection_config(\n",
    "    SCHEMA_METADATA_PATH\n",
    ")\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "    # Find output artifacts from MLMD.\n",
    "    stat_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                           SCHEMA_PIPELINE_NAME,\n",
    "                                           'StatisticsGen')\n",
    "    stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n",
    "\n",
    "    schema_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                             SCHEMA_PIPELINE_NAME,\n",
    "                                             'SchemaGen')\n",
    "    schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from StatisticsGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(stats_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(schema_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the schema for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "SCHEMA_PATH = 'schema'\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n",
    "\n",
    "# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\n",
    "shutil.copy(_generated_path, SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to view the exported schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {SCHEMA_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write model training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "# Insert the target label below\n",
    "_LABEL_KEY = None\n",
    "\n",
    "_TRAIN_BATCH_SIZE = None\n",
    "_EVAL_BATCH_SIZE = None\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int=200) -> tf.data.Dataset:\n",
    "    '''Generates features and label for training.\n",
    "    \n",
    "    Args:\n",
    "        file_pattern: List of paths or patterns of input tfrecord files.\n",
    "        data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "        schema: schema of the input data.\n",
    "        batch_size: representing the number of consecutive elements of\n",
    "            returned dataset to combine in a single batch.\n",
    "            \n",
    "    Returns:\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "            dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    '''\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY\n",
    "        ),\n",
    "        schema=schema\n",
    "    ).repeat()\n",
    "\n",
    "def _build_keras_model(schema: schema_pb2.Schema) -> tf.keras.Model:\n",
    "    '''Creates a DNN Keras model for the data.\n",
    "    \n",
    "    Returns:\n",
    "        A Keras model.\n",
    "    '''\n",
    "    # Extract features from the schema except for the target label.\n",
    "    feature_keys = [f.name for f in schema.feature if f.name != _LABEL_KEY]\n",
    "    inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n",
    "\n",
    "    # Build and compile model below\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    '''Train the model based on given args.\n",
    "    \n",
    "    Args:\n",
    "        fn_args: Holds args used to train the model as name/value pairs.\n",
    "    '''\n",
    "    # Reads in schema file passed to the Trainer component.\n",
    "    schema = tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema_pbs.Schema())\n",
    "\n",
    "    train_dataset = _input_fn(\n",
    "        fn_args.train_files,\n",
    "        fn_args.data_accessor,\n",
    "        schema,\n",
    "        batch_size=_TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    eval_dataset = _input_fn(\n",
    "        fn_args.eval_files,\n",
    "        fn_args.data_accessor,\n",
    "        schema,\n",
    "        batch_size=_EVAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model = _build_keras_model(schema)\n",
    "    model.fit(train_dataset,\n",
    "              steps_per_epoch=fn_args.train_steps,\n",
    "              validation_data=eval_dataset,\n",
    "              validation_steps=fn_args.eval_steps)\n",
    "    \n",
    "    # The result of the training should be saved in 'fn_args.serving_model_dir' directory\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str,\n",
    "                     pipeline_root: str,\n",
    "                     data_root: str,\n",
    "                     schema_path: str,\n",
    "                     module_file: str,\n",
    "                     serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "    '''Creates a pipeline using predefined schema with TFX.'''\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "        examples=example_gen.outputs['examples']\n",
    "    )\n",
    "\n",
    "    # Import the schema.\n",
    "    schema_importer = tfx.dsl.Importer(\n",
    "        source_uri=schema_path,\n",
    "        artifact_type=tfx.types.standard_artifacts.Schema\n",
    "    ).with_id('schema_importer')\n",
    "\n",
    "    # Performs anomaly detection based on statistics and data schema.\n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_importer.outputs['result']\n",
    "    )\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        module_file=module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_importer.outputs['result'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5)\n",
    "    )\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    components = [example_gen,\n",
    "                  statistics_gen,\n",
    "                  schema_importer,\n",
    "                  example_validator,\n",
    "                  trainer,\n",
    "                  pusher]\n",
    "    \n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        metadata_connection_config=sqlite_metadata_connection_config(metadata_path),\n",
    "        components=components\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell beneath runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        schema_path=SCHEMA_PATH,\n",
    "        module_file=_trainer_module_file,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        metadata_path=METADATA_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to examine the outputs of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_connection_config = sqlite_metadata_connection_config(\n",
    "    METADATA_PATH\n",
    ")\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "    ev_output=get_latest_artifacts(metadata_handler,\n",
    "                                   PIPELINE_NAME,\n",
    "                                   'ExampleValidator')\n",
    "    anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualize the anomalies from ExampleValidator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_artifacts(anomalies_artifacts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
